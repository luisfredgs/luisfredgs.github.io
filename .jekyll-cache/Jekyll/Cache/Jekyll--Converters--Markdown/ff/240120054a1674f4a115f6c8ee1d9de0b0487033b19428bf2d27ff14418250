I"ÎL<p class="image"><img src="/blog/assets/img/o-que-e-sequence-to-sequence-em-deep-learning.png" alt="" /></p>

<p>Mesmo que voc√™ s√≥ tenha come√ßado a estudar machine learning h√° pouco tempo, e ainda n√£o saiba o que √© sequence-to-sequence (seq2seq), √© quase certo de que j√° tenha ouvido falar neste termo. Trata-se de uma metodologia baseada em redes neurais que est√° presente no core de muitas aplica√ß√µes que usamos hoje em dia, e que se baseiam em intelig√™ncia artificial. Entre os mais importantes casos de uso desta metodologia, est√£o o Google Search e o Google Tradutor. H√° tamb√©m outras aplica√ß√µes interessantes, como casos de uso em modelagem de agentes conversacionais.</p>

<p>Ao final deste post, voc√™ ter√° lido sobre:</p>

<ol>
  <li>
    <p>O que √© Sequence-to-Sequence</p>
  </li>
  <li>
    <p>Como isto pode ser √∫til? Quais s√£o as aplica√ß√µes no mundo real?</p>
  </li>
  <li>
    <p>Como as redes neurais s√£o utilizadas na abordagem Sequence to Sequence?</p>
  </li>
  <li>
    <p>A arquitetura Encoder-Decoder baseada em Redes Neurais Recorrentes (RNN)</p>
  </li>
  <li>
    <p>O mecanismo Attention</p>
  </li>
  <li>
    <p>O que √© a arquitetura Transformer</p>
  </li>
</ol>

<p><em>Antes de prosseguir, <a href="https://blog.luisfred.com.br/falando-sobre-inteligencia-artificial-novo-grupo-no-slack/">participe do novo grupo no Slack</a> que criei h√° pouco tempo para promover conversas sobre Intelig√™ncia artificial, Machine Learning e Deep Learning.</em></p>

<h2 id="o-que-√©-sequence-to-sequence">O que √© Sequence-to-sequence?</h2>

<p>Mas, afinal, o que √© Sequence-to-sequence? Qual seria a teoria por tr√°s disso e quais s√£o as principais aplica√ß√µes? Na verdade, Seq2Seq √© a abordagem na qual se utiliza modelos de machine learning / deep learning para obter uma sequ√™ncia como entrada, em um dom√≠nio espec√≠fico, e converter esta sequ√™ncia para uma representa√ß√£o em outro dom√≠nio. √â trazer uma distribui√ß√£o de probabilidades de sequ√™ncias de sa√≠da e maximizar a probabilidade de uma sequ√™ncia alvo, dada uma determinada sequ√™ncia anterior como entrada. Tome como exemplo, a tradu√ß√£o de idiomas e voc√™ ter√° uma vis√£o do qu√£o importante esta abordagem tem se tornado. Na pr√°tica, acontece assim:</p>

<p><em>‚Äúle chat est noir‚Äù -&gt; <strong>[Seq2Seq]</strong> -&gt; ‚Äúthe cat is black‚Äù</em></p>

<p>Por baixo do cap√¥, √© da seguinte forma que funciona:</p>

<p class="image"><img src="https://cdn-images-1.medium.com/max/2000/0*tDxEP2_zyt79i3Ro.png" alt="Sequence to Sequence" /></p>

<h2 id="como-isto-pode-ser-√∫til-quais-s√£o-as-aplica√ß√µes-no-mundo-real">Como isto pode ser √∫til? Quais s√£o as aplica√ß√µes no mundo real?</h2>

<p>Na tradu√ß√£o neural de m√°quinas (Neural Machine Translation ), encontramos alguns exemplos de uso, n√£o apenas no ambiente de pesquisa, mas em produ√ß√£o tamb√©m (Google e Microsoft, por exemplo). Sem d√∫vidas, esta √© uma das aplica√ß√µes mais interessantes do Seq2Seq. Mas n√£o se restringe apenas √† estes casos pois, al√©m disso, pode-se aplicar esta t√©cnica em:</p>

<ol>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Question_answering">Sistemas de pergunta e resposta</a>, que respondem automaticamente √†s perguntas feitas por pessoas</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1506.05869v1.pdf">Aprendizado de agentes conversacionais</a>, como os chatbots</p>
  </li>
  <li>
    <p>Cria√ß√£o de bases de conhecimento</p>
  </li>
  <li>
    <p>Intelig√™ncias artificiais que interpretam textos (<a href="https://www.microsoft.com/en-us/research/blog/transfer-learning-machine-reading-comprehension/">Machine Reading Comprehension</a>)</p>
  </li>
  <li>
    <p><a href="https://research.google.com/pubs/pub45610.html">Tradu√ß√£o neural de m√°quina</a></p>
  </li>
</ol>

<h2 id="como-as-redes-neurais-s√£o-utilizadas-na-abordagem-sequence-to-sequence">Como as redes neurais s√£o utilizadas na abordagem Sequence to Sequence?</h2>

<p>Vou usar o Google Tradutor como refer√™ncia aqui, porque √© uma das melhores formas de se ver o potencial de toda esta ideia. Voc√™ j√° reparou que a precis√£o do Google Tradutor melhorou muito de uns tempos para c√°? A ferramenta consegue captar muito melhor o contexto das frases e nos trazer tradu√ß√µes bem melhores do que j√° proporcionara at√© pouco tempo atr√°s. Isto se deve √† aplica√ß√£o de redes neurais que deu origem √† <a href="https://blog.google/products/translate/higher-quality-neural-translations-bunch-more-languages/">segunda gera√ß√£o do tradutor deles</a>, que estamos utilizando hoje. Acontece que eles n√£o utilizam redes neurais comuns, como seria o caso das redes neurais <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network"><em>feed-foward</em></a> (alimentadas adiante). Isto porque estas arquiteturas mais b√°sicas de redes neurais n√£o trariam uma boa performance no mapeamento de sequ√™ncias. Neste caso, diferentes arquiteturas s√£o combinadas at√© que se obtenha um modelo satisfat√≥rio.</p>

<p>Todas estas aplica√ß√µes de machine learning atualmente funcionando em produ√ß√£o dentro das grandes empresas, ao menos uma boa parte delas, partiram de iniciativas na pesquisa acad√™mica. Eles estudam, fazem experimentos, comparam resultados e constatam sua viabilidade t√©cnica para uso em larga escala. √â assim com a Google, Microsoft, Facebook, IBM, etc. <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"><strong>Sutskever et al.</strong></a>, 2014 publicou um dos primeiros papers acad√™micos que mostra a abordagem Seq2Seq como sendo vi√°vel para fazer tradu√ß√£o de textos (com base em Neural Machine Translation).</p>

<p>Com um vocabul√°rio composto por 160k palavras em ingl√™s, dado como entrada para um modelo, os autores usaram uma LSTM (Long Short-Term Memory) para representar uma senten√ßa de entrada em 8k n√∫meros reais (valores cont√≠nuos). Estes 8k valores cont√≠nuos correspondem a uma representa√ß√£o intermedi√°ria, agrupados em espa√ßo vetorial de tamanho fixo. Uma segunda rede LSTM utiliza estes valores como entrada para produzir senten√ßas em Franc√™s a partir deste vetor intermedi√°rio, tamb√©m chamado de <em>vetor de contexto</em>. Os autores utilizaram redes LSTM de 4 camadas. Esta abordagem que eles adotaram √© conhecida como Encoder-Decoder.</p>

<h2 id="a-arquitetura-encoder-decoder-baseada-em-redes-neurais-recorrentes-rnn">A arquitetura Encoder-Decoder baseada em Redes Neurais Recorrentes (RNN)</h2>

<p>De uma forma resumida, o trabalho de <strong>sutskever et al. 2014</strong> contou com duas LSTMs multicamadas. Uma para mapear uma sequ√™ncia de entrada em um vetor intermedi√°rio de dimensionalidade fixa. Em seguida, vem uma outra LSTM que decodifica uma sequ√™ncia de sa√≠da a partir deste vetor. Estes componentes s√£o treinados em conjunto, para maximizar a probabilidade de uma tradu√ß√£o correta, dada uma senten√ßa de origem.</p>

<blockquote>
  <p><em>A id√©ia √© usar uma LSTM para ler a seq√º√™ncia de entrada, um timestep por vez, para obter uma grande representa√ß√£o vetorial de dimens√µes fixas, e em seguida usar outra LSTM para extrair a sequ√™ncia de sa√≠da a partir daquele vetor.</em> - <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">‚ÄúSequence to Sequence Learning with Neural Networks,‚Äù</a> 2014</p>
</blockquote>

<p><img src="https://cdn-images-1.medium.com/max/2048/0*1-FEHjs7oLced7d1.png" alt="O modelo l√™ uma senten√ßa de entrada ‚ÄúABC‚Äù e produz ‚ÄúWXYZ‚Äù como a senten√ßa de sa√≠da. Ele deixa de fazer previs√µes depois de gerar o token de fim de frase ‚ÄúEOS‚Äù- **sutskever et al. 2014**" /><em>O modelo l√™ uma senten√ßa de entrada ‚ÄúABC‚Äù e produz ‚ÄúWXYZ‚Äù como a senten√ßa de sa√≠da. Ele deixa de fazer previs√µes depois de gerar o token de fim de frase ‚ÄúEOS‚Äù- <strong>sutskever et al. 2014</strong></em></p>

<p>Trata-se de uma metodologia bastante recente, tendo se tornado mais conhecida principalmente depois que a <a href="https://research.google.com/pubs/pub45610.html">Google a adotou</a> como tecnologia principal no seu sistema de tradu√ß√£o de textos. Eles obtiveram grandes resultados ao testar esta abordagem em datasets mais famosos no que diz respeito a este dom√≠nio de tradu√ß√£o. Eles usaram o <a href="http://www.statmt.org/wmt14/translation-task.html">WMT‚Äô14</a> no benchmark.</p>

<blockquote>
  <p><em>[‚Ä¶] nossa abordagem √© transferida para conjuntos de dados de produ√ß√£o muito maiores, que t√™m v√°rias ordens de magnitude a mais de dados, para fornecer tradu√ß√µes de alta qualidade.</em> - <a href="https://arxiv.org/abs/1609.08144">Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a>, 2016</p>
</blockquote>

<p>Ap√≥s isto, surgiram muitas outras aplica√ß√µes envolvendo n√£o apenas modelos de tradu√ß√£o, mas tamb√©m em outras situa√ß√µes, como modelagem de agentes conversacionais (pode chamar de chatbots, se quiser). Neste caso, isto funcionaria como algo pr√≥ximo do seguinte:</p>

<p class="image"><img src="https://cdn-images-1.medium.com/max/2000/0*ODdjm7CjCnfbldGg.png" alt="Um exemplo de modelagem de agentes conversacionais usando redes neurais recorrentes LSTM" /><em>Um exemplo de modelagem de agentes conversacionais usando redes neurais recorrentes LSTM</em></p>

<h2 id="o-mecanismo-attention">O mecanismo Attention</h2>

<p>A arquitetura Encoder-Decoder, entretanto, traz um grande inconveniente quando surgem sequ√™ncias grandes. <a href="https://arxiv.org/pdf/1409.1259.pdf"><strong>Cho, et al.</strong></a> e <a href="https://arxiv.org/pdf/1409.0473.pdf"><strong>Bahdanau, et al.</strong></a> observaram que a performance dos modelos baseados nesta arquitetura se degrada conforme aumenta o tamanho das senten√ßas.</p>
<blockquote>
  <p><em>Uma limita√ß√£o em potencial que surge com a abordagem encoder-decoder √© que a rede neural precisa ser capaz de condensar toda a informa√ß√£o necess√°ria da senten√ßa de entrada em um vetor de dimens√µes fixas. Isto impede que a rede consiga lidar com sequ√™ncias mais longas, especialmente quando o modelo se depara com sequ√™ncias maiores do que aquelas existentes no conjunto de dados de treino.</em>  - <a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a>, bahdanau et al. 2014.</p>
</blockquote>

<p>Para contornar este problema, foi proposta uma extens√£o do encoder-decoder na forma de um mecanismo denominado <strong>Attention</strong>, no paper ‚Äú<a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a>‚Äù. A ideia √© simples, por√©m brilhante. Baseia-se no princ√≠pio de que, no lugar codificar toda a senten√ßa de entrada em um vetor de contexto de tamanho fixo, o modelo ‚Äúpresta aten√ß√£o‚Äù em partes espec√≠ficas da senten√ßa de entrada, nas quais as informa√ß√µes mais relevantes estejam concentradas. Este modelo, ent√£o, obt√©m a predi√ß√£o da palavra-alvo baseado no contexto presente no vetor intermedi√°rio, associado com estas partes espec√≠ficas da senten√ßa de entrada, bem como todas as palavras geradas em <em>steps</em> anteriores.</p>
<blockquote>
  <p><em>A mais importante diferen√ßa entre esta abordagem e a arquitetura encoder-decoder b√°sica √© que ela n√£o tenta codificar toda a senten√ßa de entrada em um simples vetor de tamanho fixo.</em> - <a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a>, bahdanau et al. 2014.</p>
</blockquote>

<p>O modelo de <a href="https://arxiv.org/pdf/1409.0473.pdf"><strong>Bahdanau, et al.</strong></a> tamb√©m se baseia em redes neurais recorrentes, mas no lugar de usar LSTM, eles usaram a arquitetura <a href="https://arxiv.org/pdf/1406.1078v3.pdf">GRU</a> <strong>(Cho, et al., 2014).</strong> Al√©m disso, o modelo traz uma t√©cnica genial que promove um alinhamento entre a senten√ßa de entrada e a senten√ßa de sa√≠da correspondente.</p>

<p class="image"><img src="https://cdn-images-1.medium.com/max/2000/0*cjkzXnJQCagZFZoA.png" alt="" />
<em>Os eixos x e y de cada gr√°fico correspondem √†s palavras na senten√ßa de origem (Ingl√™s) e a tradu√ß√£o obtida pelo modelo (Franc√™s), respectivamente. As diferentes intensidades dos pixels mostram o quanto uma palavra na senten√ßa de origem corresponde com outra na senten√ßa alvo. Isto √© usado como crit√©rio para alinhar as senten√ßas antes de obter a tradu√ß√£o.</em></p>

<p>E se voc√™ quiser <strong>meter a m√£o no c√≥digo</strong> e testar esta implementa√ß√£o em algum framework de deep learning, basta<a href="http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"> acessar este tutorial</a> na p√°gina de documenta√ß√£o do framework <a href="http://pytorch.org/">Pytorch</a>. Trata-se de um dos frameworks de deep learning ‚Äúmais lindos‚Äù que eu j√° vi, sendo muito indicado para o ambiente de pesquisas. O Pytorch √© bem menos verboso do que o <a href="https://www.tensorflow.org/">tensorflow</a> e isto ajuda na interpreta√ß√£o das implementa√ß√µes s√≥ de bater o olho no c√≥digo.</p>

<h2 id="a-arquitetura-transformer">A arquitetura Transformer</h2>

<p>Redes neurais recorrentes costumam ser computacionalmente intensivas, dada a sua natureza sequencial. A rigor, quanto mais informa√ß√£o a RNN precisar processar at√© captar o contexto de uma sequ√™ncia, mais recursos computacionais ela ir√° consumir. Ao mesmo tempo, isto se torna uma barreira ao aproveitamento eficaz da paraleliza√ß√£o em GPUs por parte destas arquiteturas. √â algo que vem a ser mais cr√≠tico em sequ√™ncias longas e em datasets maiores.</p>

<p>T√©cnicas mais recentes proporcionaram melhorias significativas com respeito √† efici√™ncia computacional e na performance dos modelos baseados nestas arquiteturas recorrentes, como √© o caso do <strong>Attention</strong>. Mesmo assim, estes m√©todos ainda n√£o aproveitam bem a paraleliza√ß√£o em GPUs e carecem, portanto, de performance. <a href="https://blog.luisfred.com.br/reconhecimento-de-escrita-manual-com-redes-neurais-convolucionais/">Redes neurais Convolucionais</a> (CNNs) s√£o menos sequenciais do que as RNNs mas, ainda assim, o n√∫mero de passos necess√°rios para computar informa√ß√µes de longo prazo em uma sequ√™ncia aumenta com a dist√¢ncia entre estas informa√ß√µes. Isto ainda pode comprometer bastante a performance e a efici√™ncia computacional destes modelos.</p>

<p><a href="https://arxiv.org/abs/1706.03762"><strong>Vaswani et al. (2017)</strong></a> prop√¥s uma nova arquitetura neural que evita completamente esta depend√™ncia das RNNs, e tamb√©m CNNs. No lugar disto, eles contaram apenas com um mecanismo <a href="https://arxiv.org/pdf/1703.03130.pdf"><em>self-attention</em></a> e redes feed-foward. O novo m√©todo, denominado <strong>Transformer</strong>, √© <a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html">computacionalmente menos exigente</a> e supera a performance dos m√©todos anteriores, baseados em RNNs e CNNs. A Transformer modela, de uma forma direta, os relacionamentos entre todas as palavras de um senten√ßa, independente do posicionamento de cada uma delas.</p>

<p>Para computar a representa√ß√£o de uma dada palavra em uma sequ√™ncia, a Transformer compara esta palavra com cada uma das outras presentes na senten√ßa. Como resultado, obt√©m-se um score para cada uma das palavras j√° computadas, sendo que esta pontua√ß√£o determina o quanto cada termo anterior contribui para a representa√ß√£o do pr√≥ximo.</p>

<p class="image"><img src="https://cdn-images-1.medium.com/max/2000/0*I2ng-cUnE3Rdm9O8.png" alt="A Transformer possui um stack de 6 camadas id√™nticas. No encoder, cada camada possui duas sub-camadas. A primeira implementa uma varia√ß√£o do mecanismo self-attention, chamado Multi-Head self-attention. A segunda √© um simples rede feed-foward. O decoder tamb√©m possui duas sub-camadas, mas ainda insere uma terceira, a qual aplica o m√©todo attention em toda a sa√≠da do encoder." />
<em>A Transformer possui um stack de 6 camadas id√™nticas. No encoder, cada camada possui duas sub-camadas. A primeira implementa uma varia√ß√£o do mecanismo self-attention, chamado Multi-Head self-attention. A segunda √© um simples rede feed-foward. O decoder tamb√©m possui duas sub-camadas, mas ainda insere uma terceira, a qual aplica o m√©todo attention em toda a sa√≠da do encoder.</em></p>

<p>Mais detalhes desta abordagem podem ser obtidos por meio da leitura do paper ‚Äú<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>‚Äù <strong>(Vaswani et al. 2017).</strong> Os autores tamb√©m <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py">disponibilizaram o c√≥digo-fonte</a> em tensorflow, usado nos experimentos. Voc√™ pode tentar reproduzir os resultados que eles obtiveram. Existe uma implementa√ß√£o deste mecanismo usando o framework Pytorch e que √© bem mais f√°cil de interpretar, <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">veja neste link</a>.</p>

<h2 id="para-concluir">Para concluir</h2>

<p>Durante praticamente todo o post eu citei o exemplo da tradu√ß√£o neural de textos, mas voc√™ tamb√©m viu que Seq2Seq pode ser aplicado em outros problemas de NLP (Natural Language Processing). Foi dito que isto tamb√©m pode ser aplicado em Chatbots. Isto √© interessante, porque no lugar de termos respostas previamente selecionadas para situa√ß√µes espec√≠ficas, como ocorre nos CB baseados em regras, estas respostas seria exibidas completamente com base em crit√©rios probabil√≠sticos. Estas respostas seriam escolhidas pelo pr√≥prio chatbot para responder quest√µes espec√≠ficas. √â algo que o deixaria mais aut√¥nomo e inteligente.</p>

<p>A cria√ß√£o de bases de conhecimento √© tamb√©m uma aplica√ß√£o interessante. Imagine uma I.A que indexa o conte√∫do de um blog como este e automaticamente cria uma se√ß√£o FAQ com perguntas e respostas completamente baseadas no conte√∫do encontrado? Isto j√° est√° sendo feito em larga escala, com o uso de seq2seq.</p>

<p>Agora, me fala uma coisa a√≠ nos coment√°rios! O que foi que voc√™ achou deste post? Te ajudou a entender um pouco sobre a teoria seq2seq, que voc√™ precisa para criar suas aplica√ß√µes baseadas em NLP e I.A? Comenta a√≠!</p>

<h2 id="refer√™ncias-para-um-estudo-mais-aprofundado">Refer√™ncias, para um estudo mais aprofundado</h2>

<p><a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence Learning with Neural Networks</a></p>

<p><a href="https://arxiv.org/pdf/1406.1078v3.pdf">Learning Phrase Representations using RNN Encoder‚ÄìDecoder for Statistical Machine Translation</a></p>

<p><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate</a></p>

<p><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></p>

<p><a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a></p>

<p><a href="https://nlp.stanford.edu/manning/talks/SIGIR2016-Deep-Learning-NLI.pdf">Natural Language Inference, Reading Comprehension and Deep Learning</a></p>

<p><a href="https://arxiv.org/abs/1609.08144">Google‚Äôs Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a>, 2016</p>

<p><a href="https://arxiv.org/pdf/1506.05869v1.pdf">A Neural Conversational Model</a></p>
:ET